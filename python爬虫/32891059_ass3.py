# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XeJ1ultQSpQGHK-XUj620xLwqRCQHZLM

# assignment analysis

assignment analysis: Suburbs in different lists need to be combined to calculate where the nearest bus stop is for each place. Since it is necessary to arrive at Melbourne Central Railway Station within 7-9am, it is first necessary to screen the buses that arrive at this station during this time period and calculate the time of each stop that can arrive at this station. If these buses pass through the nearest bus stop of the residence, there will be direct buses. The web data is then crawled to store the content on the site into a form, which is then combined with the existing form to produce the final output file.

# Libraries to be used

The following are the libraries used<br>
pandas: Used for data processing and analysis<br>
Xml.etree.elementtree: Used to parse and manipulate XML data<br>
urllib.request: Used to send HTTP requests and obtain network data<br>
bs4.BeautifulSoup: Used to parse HTML data<br>
Http.cookiejar: For handling HTTP cookies<br>
time: used for time-related operations<br>
geopandas: Used for geographic data processing and analysis<br>
shapely.Geometry.Point: A geometric object used to represent a geographic point<br>
numpy: Used for scientific computation and array manipulation<br>
scipy.stats: Used for statistical analysis and probability distribution<br>
sklearn.model_selection.train_test_split: Used to divide the training and test sets<br>
sklearn.linear_model.LinearRegression: Used for linear regression models<br>
sklearn.metrics.r2_score: Used to calculate the R^2 score<br>
matplotlib.pyplot: Used for data visualization<br>
Sklearn. Preprocessing. StandardScaler: for characteristics of standardization<br>
Sklearn. Preprocessing. FunctionTransformer: used for custom data conversion function<br>
Yellowbrick. Regressor. Residuals_plot: used to map regression model of residual error<br>
Sklearn. Preprocessing. MinMaxScaler: used for feature scaling<br>
math.radians: A function that converts angles to radians.<br>
math.sin: A function that calculates the sine value of a given Angle.<br>
math.cos: A function that calculates the cosine of a given Angle.<br>
math.asin: A function that calculates the arcsine value of a given value.<br>
math.sqrt: A function that calculates the square root of a given value.<br>
pipeline: used to combine data preprocessing steps and linear regression models.<br>
extract_text:Used to read pdf files<br>
"""

import pandas as pd
import xml.etree.ElementTree as ET
from urllib.request import urlopen, build_opener, HTTPCookieProcessor
from bs4 import BeautifulSoup
import http.cookiejar
import time
import geopandas as gpd
from shapely.geometry import Point
import numpy as np
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import MinMaxScaler
from math import radians, sin,cos, asin, sqrt
from sklearn.pipeline import Pipeline
from pdfminer.high_level import extract_text
from yellowbrick.regressor import ResidualsPlot
from sklearn.preprocessing import PowerTransformer

"""# 1.The location is merged with the block downtown file

## 1.1Read and parse the file
"""

xml_file_path = '32891059.xml'

# Read XML file
with open(xml_file_path, 'r') as file:
    xml_data = file.read()

# Preprocess XML data
replace_dict = {'&': '&amp;', '"': '&quot;', "'": '&apos;'}
xml_data = xml_data.translate(str.maketrans(replace_dict))
xml_data = '<root>\n' + xml_data + '\n</root>'

# Parse XML data
root = ET.fromstring(xml_data)

# Extract data from XML
data = []
for pe in root:
    property_id = pe.find('property_id').text
    lat = pe.find('lat').text
    lng = pe.find('lng').text
    addr_street = pe.find('addr_street').text
    data.append([property_id, lat, lng, addr_street])

# Create XML DataFrame
columns = ['property_id', 'lat', 'lng', 'addr_street']
df_xml = pd.DataFrame(data, columns=columns)

df_xml

json_file_path = '32891059.json'
# Read JSON file
df_json = pd.read_json(json_file_path)

# Display the DataFrame
df_json

"""## 1.2Merge json and xml file content"""

# Merge and remove duplicates
merged_df = pd.concat([df_xml, df_json], ignore_index=True)
merged_df = merged_df.drop_duplicates()

# Display the merged DataFrame
merged_df

"""## 1.3Combined suburb"""

shapefile_path = "VIC_LOCALITY_POLYGON_shp.shp"

# Load shapefile into a GeoDataFrame
gdf = gpd.read_file(shapefile_path)

# Create spatial index
spatial_index = gdf.sindex

# Create a copy of merged_df
df1 = merged_df.copy()
df1['lat'] = df1.lat.apply(lambda x: float(x))
df1['lng'] = df1.lng.apply(lambda x: float(x))

# Assign suburb based on lng/lat using spatial index
for index, row in df1.iterrows():
    sub_lng = row['lng']
    sub_lat = row['lat']
    
    # Create a point geometry from lng/lat
    point = Point(sub_lng, sub_lat)
    
    # Query the spatial index for potential matches
    matches = list(spatial_index.intersection(point.bounds))
    
    # Check if the point is contained in any of the shapes
    for match in matches:
        if gdf.geometry.iloc[match].contains(point):
            df1.loc[index, 'suburb'] = gdf['VIC_LOCA_2'].iloc[match]
            break
    else:
        df1.loc[index, 'suburb'] = None

# Create another copy of df1
df2 = df1.copy()

# Display df2
df2

"""## 1.4Combined lga"""

from pdfminer.high_level import extract_text
import pandas as pd

# Extract text from PDF
lga = extract_text('Lga_to_suburb.pdf')
lga = lga.upper()

# Split the text into regions and suburb lists
region_list = [region.strip() for region in lga.split('\n\n') if region.strip()]

# Create a list of dictionaries for each region and suburb
data = [{'lga': region.split(' : ')[0], 'suburb': eval(region.split(' : ')[1])} for region in region_list]

# Create a DataFrame from the list of dictionaries
lag_to_suburb = pd.DataFrame(data)
lag_to_suburb

lag_to_suburb = lag_to_suburb.explode('suburb')
lag_to_suburb

# Merge using the 'suburb' column
df1 = pd.merge(df2, lag_to_suburb, on='suburb')

# Delete duplicate lines in df1
df1 = df1.drop_duplicates(subset='property_id')

# Re-index df1 according to the new property_id order
new_property_id_order = df2['property_id'].tolist()
df1 = df1.set_index('property_id')
df1 = df1.reindex(new_property_id_order)
df1 = df1.reset_index()

"""# 2. Process data, calculate time distance from the site, etc

## 2.1 Calculate the nearest station
"""

#Import the math library

#The function calculates the distance between two latitudes and longitudes
#Enter the latitude and longitude of the two locations
#Output the calculated distance
def calculate_distance(lat1,lon1,lat2,lon2):
    # Convert the Angle to radians
    lon1,lat1,lon2,lat2 = map(radians, [float(lon1),float(lat1),float(lon2),float(lat2)])
    # Difference between latitude and longitude
    difference_lon = lon2 -lon1
    difference_lat = lat2 -lat1
    # Haversine formula
    spherical_distance_square = sin(difference_lat/2)**2+cos(lat1)*cos(lat2)*sin(difference_lon/2)**2
    actual_distance = 2*asin(sqrt(spherical_distance_square))
    # Mean radius of the Earth 
    radius = 6378
    # Return distance
    return actual_distance *radius

stops = pd.read_csv('stops.txt')
stops

def distance_to_closest(customer_location):
    """
    Calculate the distance to the nearest warehouse and the warehouse ID

    Parameters:
    customer_location (tuple): Latitude and longitude of the customer's location in format (lat, lon)

    Return value:
    tuple: A tuple containing the nearest distance and the nearest warehouse ID in the format (closest_distance, closest_id).
    """
    lat, lon = customer_location

    # Calculate the distance of each bus stop from the customer's location
    stops['distance'] = np.vectorize(calculate_distance)(
        lat, lon, stops['stop_lat'], stops['stop_lon']
    )

    # Find the nearest distance and the corresponding bus stop
    closest_idx = stops['distance'].idxmin()
    closest_distance = stops.at[closest_idx, 'distance']
    closest_id = int(stops.at[closest_idx, 'stop_id'])

    return closest_distance, closest_id

df1[['distance_to_closest_train_station', 'closet_train_station_id']] = df1.apply(
    lambda row: pd.Series(distance_to_closest((row['lat'], row['lng']))), axis=1
)

# Converts closet_train_station_id to an integer type
df1['closet_train_station_id'] = df1['closet_train_station_id'].astype(int)

# Leave the distance_to_closest_train_station column with 9 decimal places
df1['distance_to_closest_train_station'] = df1['distance_to_closest_train_station'].round(9)

df1

"""## 2.2Screening bus routes that meet the requirements"""

calendar = pd.read_csv('calendar.txt')
#Find buses that operate Monday through Friday
filtered_calendar = calendar[(calendar.iloc[:, 1:6] == 1).all(axis=1)]

filtered_calendar

# Find the name of the stop to Melbourne Central Railway Station
filtered_stops = stops[stops['stop_name'] == 'Melbourne Central Railway Station']

filtered_stops

stop_times = pd.read_csv('stop_times.txt', sep=',')
busid = stop_times['trip_id']
busid

clear_bus_id = []

for i in busid:
    # Cut the string to get the id of the bus
    clear_bus_id.append(i.split('.')[1])
stop_times['car_id'] = clear_bus_id

stop_times

trips = pd.read_csv('trips.txt')
trips

# Select the row where 'car_id' is 'T0'
filtered_stops = stop_times[stop_times['car_id'] == 'T0']
# Screening arrival time 7-9am and stop_id is 19842
filtered_stops1 = filtered_stops[(filtered_stops['stop_id'] == 19842) & (filtered_stops['departure_time'].str[:2].isin(['07', '08']))]
filtered_stops2 =   filtered_stops[(filtered_stops['stop_id'] == 19842) & (filtered_stops['departure_time']=='09:00:00')]

new_filtered_stops = pd.concat([filtered_stops1, filtered_stops2], ignore_index=True)

#Check out the Walk to 19842 site
df1[df1['closet_train_station_id'] == 19842]

#Check which buses take you directly
trid =new_filtered_stops['trip_id']
set(trid)

#Sifting through bus data for direct transit
trip_can = filtered_stops[filtered_stops['trip_id'].isin(set(trid))].reset_index(drop=True)

trip_can.head()

"""## 2.3Calculate the arrival time of Melbourne Central Railway Station"""

def calculate_cal_time(df):
    """
    Used to calculate how many branches are needed between each site and 19842
    input:df
    output:df
    """
    df['departure_time'] = pd.to_datetime(df['departure_time'])
    last_departure_time = df.iloc[-1]['departure_time']
    df['cal_time'] = (last_departure_time - df['departure_time']).dt.total_seconds() / 60
    df['cal_time'] = df['cal_time'].astype(int)
    df['departure_time'] = df['departure_time'].dt.strftime('%H:%M:%S') 
    return df

clean_direct_access = pd.DataFrame()
def select_data(test):
    """
    Select the data from the first row to the row with the value 19842
    input:test
    return: subset_result
    """
    # Copy a subset of the test dataset until the condition 'stop_id' is 19842
    subset = test.loc[:test.index[test['stop_id'] == 19842][0]].copy()
    subset_result = calculate_cal_time(subset)

    return subset_result

# Go through all the bus routes
for i in set(trid):
    test = trip_can[trip_can['trip_id'] == i].copy()

    if clean_direct_access.empty:
        clean_direct_access = select_data(test)
    else:
        clean_direct_access = pd.concat([clean_direct_access, select_data(test)])

clean_direct_access

clean_direct_access['cal_time'] = clean_direct_access['cal_time'].astype(int)
clean_direct_access['travel_min_to_MC'] = clean_direct_access.groupby('stop_id')['cal_time'].transform('mean').round().astype(int)

df1 = pd.merge(df1, clean_direct_access[['stop_id', 'travel_min_to_MC']], left_on='closet_train_station_id', right_on='stop_id', how='left')
df1.drop_duplicates(keep='last', inplace=True)

df1.info()
#Only 1994 can be observed that can be reached directly

#Handle those that cannot be reached directly and create a direct_journey_flag
df1['travel_min_to_MC'] = df1['travel_min_to_MC'].fillna(-1)
df1['travel_min_to_MC'] =df1['travel_min_to_MC'].astype(int)
df1['direct_journey_flag'] = df1['travel_min_to_MC'].apply(lambda x: 0 if x == -1 else 1)
df1.drop(['stop_id'], axis=1, inplace=True)
df1.loc[df1['travel_min_to_MC'] == -1, 'travel_min_to_MC'] = 'no direct trip is available'

df1

"""# 3. Crawl web data

## 3.1Ready to climb the list
"""

# Organize the suburbs that require a website request
list_suburb = df1['suburb']
list_suburb = set(list_suburb)
list_suburb = list(list_suburb)  
for i in range(len(list_suburb)):
    if " " in list_suburb[i]:
        list_suburb[i] = list_suburb[i].replace(" ", "+")
print(list_suburb)

"""## 3.2Crawl data"""

#Request the URL and get the data in the table
from urllib.request import urlopen, build_opener, HTTPCookieProcessor
from bs4 import BeautifulSoup
import http.cookiejar
import time

cookie_jar = http.cookiejar.CookieJar()
cookie_processor = HTTPCookieProcessor(cookie_jar)
opener = build_opener(cookie_processor)
all_content = []
for i in list_suburb:
    response = opener.open(f'http://house.speakingsame.com/profile.php?q={i}%2C+VIC')
    html = response.read()
    soup = BeautifulSoup(html, 'html.parser')
    all_content.append(soup.find_all('table'))
    time.sleep(10)

"""## 3.3Filter the crawled data"""

#Creates an array for storing variables
municipality=[]
number_of_houses =[]
number_of_units=[]
aus_born_perc=[]
median_house_price = []
median_income = []
population = []
# Iterate through each <table> tag in all_content
for table_content in all_content:
    # Converts table_content to a string
    table_content_str = str(table_content)
    
    # Parse the contents of the current <table> tag using BeautifulSoup
    soup = BeautifulSoup(table_content_str, 'html.parser')
    #Data addition is restricted because multiple groups of duplicate data are found
    limit1 = 0
    limit2 = 0
    limit3 = 0
    limit4 = 0
    limit5 = 0
    limit6 = 0
    # Extract the contents of the <td> tag
    td_tags = soup.find_all('td')
    td_tags = [tag for tag in td_tags if tag.text.strip() != '']
    # Filter the required data and add the corresponding array
    for i in range(len(td_tags)):
        if td_tags[i].get_text() == 'House' and limit6 == 0:
            limit6 = 1
            median_house_price.append(td_tags[i+1].get_text())
        if td_tags[i].get_text() == 'Municipality' and limit1 == 0:
            limit1 = 1
            municipality.append(td_tags[i+1].get_text())
        if td_tags[i].get_text() == 'Number of houses/units' and limit2 == 0:
            limit2 = 1
            number_of_houses.append(td_tags[i+1].get_text().split('/')[0].strip())
            number_of_units.append(td_tags[i+1].get_text().split('/')[1].strip())
        if  td_tags[i].get_text() == 'All People'and limit3 == 0:
            limit3 = 1
            population.append(td_tags[i+1].get_text())
        if td_tags[i].get_text() == 'Australian Born'and limit4 == 0:
            limit4 = 1
            aus_born_perc.append(td_tags[i+1].get_text())
        if td_tags[i].get_text() =='Weekly income'and limit5 == 0:
            limit5 = 1
            median_income.append(td_tags[i+1].get_text())

# Delete the + sign
for i in range(len(list_suburb)):
    if "+" in list_suburb[i]:
        list_suburb[i] = list_suburb[i].replace("+", " ")

#Generate a web_csv1 used to generate a table to store data in the web page
web_csv1 = pd.DataFrame()
web_csv1['suburb']=list_suburb
web_csv1['number_of_houses']=number_of_houses
web_csv1['number_of_units']=number_of_units
web_csv1['municipality'] = municipality
web_csv1['aus_born_perc'] = aus_born_perc
web_csv1['median_income']=median_income
web_csv1['median_house_price']=median_house_price 
web_csv1['population']=population

web_csv1

"""# 4.Output file

## 4.1Integrated file
"""

#Combine web_csv1 and df1
df1['closet_train_station_id'] = df1['closet_train_station_id'].astype('int64')
df1 = df1.rename(columns={'closet_train_station_id': 'closest_train_station_id'})
merged_df = pd.merge(df1, web_csv1, on='suburb', how='left')
merged_df.info()
print(merged_df.columns)

#Sort the columns in the table
new_columns = ['property_id', 'lat', 'lng','addr_street','suburb','number_of_houses','number_of_units','municipality','population','aus_born_perc','median_income','median_house_price','lga','closest_train_station_id','distance_to_closest_train_station','travel_min_to_MC','direct_journey_flag']
merged_df = merged_df.reindex(columns=new_columns)

merged_df.info()

"""## 4.2View sample file"""

#Import the sample_output file and view its properties for each column
sample_csv= pd.read_csv('sample_output.csv')
sample_csv.info()

"""## 4.3Change the file format to the same as the sample file"""

#Change the properties of a column
merged_df['property_id'] = merged_df['property_id'].astype('int64')
merged_df['number_of_houses'] = merged_df['number_of_houses'].astype('float64')
merged_df['number_of_units'] = merged_df['number_of_units'].astype('float64')
merged_df['population'] = merged_df['population'].astype('float64')

merged_df.info()

"""## 4.4Output file"""

merged_df.to_csv('32891059_A3_solution.csv', index=False)

"""# 5.Linear regression model

## 5.1Prepare the required data
"""

reshape_data = merged_df.iloc[:,[5,6,8,9,10,11]]
reshape_data

for i in range(len(reshape_data)):
    reshape_data.loc[i,'median_income'] = int(reshape_data.loc[i,'median_income'].split('$')[1].replace(',',''))
    reshape_data.loc[i,'median_house_price'] = int(reshape_data.loc[i,'median_house_price'].replace(',','').split('$')[1])
    reshape_data.loc[i,'aus_born_perc'] = int(reshape_data.loc[i,'aus_born_perc'].split('%')[0])
reshape_data = reshape_data.astype({"number_of_houses":"int","number_of_units":"int","population":"int","median_house_price":"int",'aus_born_perc':'int'})
reshape_data = reshape_data.astype({'median_income': "int", 'median_house_price': "int"})
reshape_data

"""## 5.2Computational linear regression"""

# Preprocessing steps such as normalization/conversion are performed
preprocessor = Pipeline([
    ('scaler', StandardScaler()),  # Normalization step
])

data = reshape_data  # Replace 'reshape_data' with your actual DataFrame

x = data.iloc[:, 0:5]
y = data.iloc[:, 5]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=10)

# Use preprocessing steps in linear regression models
model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression()),
])

model.fit(x_train, y_train)
y_predict = model.predict(x_test)

print('AS linear model R-squared:', r2_score(y_test, y_predict))

"""## 5.3Draw image"""

#When printing out, you need to control the size of the image
plt.figure(figsize=(10, 10))
reshape_data.hist()
plt.show()

"""## 5.4Four different linear models were used for the analysis

### 5.4.1Analyze using the standardization model
"""

#standardization
scaler = StandardScaler()
StandardScaler = pd.DataFrame(scaler.fit_transform(reshape_data))
x2 = StandardScaler.iloc[:, 0:5]
y2 = StandardScaler.iloc[:, 5]
x2

# Divide the training set and test set
x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.1, random_state=10)

# Create and train linear regression model objects
linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)

# Make predictions on the test set
y_pred = linear_reg.predict(x_test)

# Calculate the R-squared value
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)

# Create residuals graph objects and fit them
residuals_plot = ResidualsPlot(linear_reg, hist=False, qqplot=True)
residuals_plot.fit(x2, y2)
residuals_plot.show()

"""### 5.4.2The min-max normalization model is used for its analysis"""

scaler = MinMaxScaler()
min_max_scaled = pd.DataFrame(scaler.fit_transform(reshape_data))
x3 = min_max_scaled.iloc[:, 0:5]
y3 = min_max_scaled.iloc[:, 5]
x3

# Divide the training set and test set
x_train, x_test, y_train, y_test = train_test_split(x3, y3, test_size=0.1, random_state=10)

#Create and train linear regression model objects
linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)

# Make predictions on the test set
y_pred = linear_reg.predict(x_test)

# Calculate the R-squared value
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)

# Create residuals graph objects and fit them
residuals_plot = ResidualsPlot(linear_reg, hist=False, qqplot=True)
residuals_plot.fit(x3, y3)
residuals_plot.show()

"""### 5.4.3Use the log model for analysis"""

# Logarithmic conversion of data
transformer = FunctionTransformer(np.log)
log_data = pd.DataFrame(transformer.transform(reshape_data))

# Filter valid data
x4 = log_data.iloc[:, 0:5]
indices_to_keep = ~x4.isin([np.nan, np.inf, -np.inf, 0]).any(1)
x4 = x4[indices_to_keep].astype(np.float64).reset_index(drop=True)

# Filter target variable
y4 = log_data.iloc[:, 5]
y4 = y4[indices_to_keep].reset_index(drop=True)
x4

# Divide the training set and test set
x_train, x_test, y_train, y_test = train_test_split(x4, y4, test_size=0.1, random_state=10)

# Create and train linear regression model objects
linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)

# Make predictions on the test set
y_pred = linear_reg.predict(x_test)

# Calculate the R-squared value
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)

# Create residuals graph objects and fit them
residuals_plot = ResidualsPlot(linear_reg, hist=False, qqplot=True)
residuals_plot.fit(x4, y4)
residuals_plot.show()

"""### 5.4.4The power model was used for analysis"""

# Power conversion of the data
power_transformer = PowerTransformer()
power_data = pd.DataFrame(power_transformer.fit_transform(reshape_data))

# Select feature variables and target variables
x5 = power_data.iloc[:, 0:5]
y5 = power_data.iloc[:, 5]
x5

# Divide the training set and test set
x_train, x_test, y_train, y_test = train_test_split(x5, y5, test_size=0.1, random_state=10)

# Create and train linear regression model objects
linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)

# Make predictions on the test set
y_pred = linear_reg.predict(x_test)

# Calculate the R-squared value
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)

# Create residuals graph objects and fit them
residuals_plot = ResidualsPlot(linear_reg, hist=False, qqplot=True)
residuals_plot.fit(x5, y5)
residuals_plot.show()

"""### 5.4.5The box-cox model was used for analysis"""

# The Box-Cox method was used for data transformation
box_transformer = PowerTransformer(method='box-cox', standardize=False)
box_data = pd.DataFrame(box_transformer.fit_transform(reshape_data))

# Select feature variables and target variables
x6 = box_data.iloc[:, 0:5]
y6 = box_data.iloc[:, 5]
x6

# Divide the training set and test set
x_train, x_test, y_train, y_test = train_test_split(x6, y6, test_size=0.1, random_state=10)

# Create and train linear regression model objects
linear_reg = LinearRegression()
linear_reg.fit(x_train, y_train)

# Make predictions on the test set
y_pred = linear_reg.predict(x_test)

# Calculate the R-squared value
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)

# Create residuals graph objects and fit them
residuals_plot = ResidualsPlot(linear_reg, hist=False, qqplot=True)
residuals_plot.fit(x6, y6)
residuals_plot.show()

"""Based on the given results, we observe the following R-squared values:

The R-squared value (AS linear model) of the raw data is 0.723, indicating a moderately linear relationship between the feature and the target variable.
The R-squared value of the normalized data is the same as that of the original data, indicating that standardization has no significant effect on the linear relationship.
The R-squared value of the data after MinMax scaling is similar to that of the original data, indicating that MinMax scaling maintains a linear relationship.
After applying logarithmic transformation to the data, the R-squared value increases to 0.769, which indicates that logarithmic data enhances the linear relationship between the feature and the target variable.
In addition, we used the PowerTransformer and Box-Cox transformation methods and observed the following results:

After the transformation with PowerTransformer, the R-squared value is 1.0, indicating a complete linear relationship between the feature and the target variable.
After using Box-Cox transformation, R-squared value is 0.766, indicating a good linear relationship between the feature and the target variable.
Based on the observation, we can draw the following conclusions:

Normalization and MinMax scaling have little effect on the linear relationship and do not significantly change the linear relationship between the feature and the target variable.
log transformation is good at enhancing the linear relationship between features and target variables.
PowerTransformer and Box-Cox transformations can also enhance linear relationships, but may introduce the risk of overfitting and require further validation.
In summary, in this task, for developing a linear regression model to predict median home prices, a log transformation may be the most appropriate data preprocessing method because it enhances the linear relationship between features and target variables.

# 6.references

[1]https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html For reference linear regression equation writing<br>
[2]https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html Used in reference to StandardScaler linear regression equation formulation<br>
[3]https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html For reference to MinMaxScaler linear regression equation writing<br>
[4]https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html Used for reference function transformation notation<br>
[5]https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html For reference Power linear regression equation writing<br>
[6]https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html For reference boxcox linear regression equation writing<br>
"""